# Red_Neuronal-para-Vision_Artificial-con-Logica_difusa
Proyecto en Python que implementa una red neuronal convolucional (CNN) para tareas de visi√≥n artificial, integrando l√≥gica difusa para decisiones interpretables. Incluye entrenamiento profundo, preprocesamiento de im√°genes y evaluaci√≥n de desempe√±o.
## üì¶ Dataset

Este proyecto utiliza el dataset [Cats vs Dogs de Kaggle](https://www.kaggle.com/datasets/sansin457/cats-vs-dogs) para tareas de clasificaci√≥n binaria. El dataset no est√° incluido en este repositorio por motivos de licencia. Para acceder a √©l, visita el enlace y acepta los t√©rminos de uso en Kaggle.
# üß† Red Neuronal para Visi√≥n Artificial con L√≥gica Difusa

Proyecto en Python que implementa una red neuronal convolucional (CNN) para clasificaci√≥n binaria de im√°genes (gatos vs perros), integrando l√≥gica difusa para decisiones interpretables.


---

## üöÄ Instalaci√≥n

1. Clona el repositorio:
```bash
git clone https://github.com/tu_usuario/tu_repositorio.git
cd tu_repositorio
```
2. Crea y activa tu entorno virtual:
‚ùópara este proyecto es necesario usar una version de python compatible con TensorFlow qeu sea estable, como la 3.10.11‚ùó
```bash
   py -3.10 -m venv .venv 
   python --version # verifica que la salida en consola sea 'Python 3.10.11' o algo similar
   source .venv/bin/activate  # o .venv\Scripts\activate en Windows
```
3. Instala las dependencias:
```bash
    pip install -r requirements.txt
```   

## üîÑ Aumento de datos

El proyecto incluye scripts para aplicar t√©cnicas de aumento de datos sobre el conjunto de im√°genes, como rotaci√≥n, volteo horizontal, escalado y ajustes de brillo. Estas transformaciones permiten generar nuevas muestras a partir de las im√°genes originales, lo cual es especialmente √∫til cuando se dispone de una cantidad limitada de datos.

El aumento de datos mejora la capacidad de generalizaci√≥n del modelo, reduce el riesgo de sobreajuste y simula condiciones m√°s variadas del mundo real. Esta etapa se ejecuta antes del entrenamiento y est√° integrada en el flujo de preprocesamiento del proyecto.


## üß† Contexto te√≥rico
# Redes Neuronales Convolucionales(CNN)
Las CNN's con arquitecturas especializadas en procesamiento de im√°genes que funcionan mediante:

-Convoluci√≥n: Operaci√≥n que permite detectar patrones espec√≠ficos como bordes o texturas, al multiplicar y sumar valores de los p√≠xeles bajo el filtro mientras este se desliza por toda la imagen. Estos filtros se entrenan para identificar caracter√≠sticas distintas y aplicar m√∫ltiples filtros en capas convolucionales la red puede aprender representaciones jer√°rquicas de las im√°genes. 
Para este proyecto, en Keras, la clase Conv2D, permite definir el n√∫mero de filtros, el tama√±o y su funci√≥n de activaci√≥n (ReLU) que en este caso nos ayud√≥ a introducir no linealidades y mejorar la capacidad de nuestra red para aprender patrones mas complicados.

-Pooling: Una pr√°ctica de la estad√≠stica para agrupar o combinar datos para facilitar su manejo, an√°lisis o uso eficiente. En este caso, esta operaci√≥n es para reducir dimensionalidad espacial en las im√°genes o matrices resultantes tras aplicar convoluciones. No confundir con el redimensionar im√°genes. El objetivo principal de hacer esto es agrupar valores num√©ricos, reducir la cantidad de datos y agilizar los entrenamientos y procesos en los modelos de Deep Learning (DL).

-Capas densas - totalmente conectadas: act√∫an como la parte de clasificaci√≥n o regresi√≥n de la red. Detr√°s de las capas convolucionales, que extraen caracter√≠sticas jer√°rquicas de los datos de entrada, las capas densas procesan estas caracter√≠sticas para realizar tareas como la clasificaci√≥n de im√°genes final. Las entradas en estas capas vienen de las capas convolucionales y de pooling, que ya han reducido y resumido la informaci√≥n espacial; antes de llegar a estas capas se suele aplicar un funci√≥n de aplanamiento que pasa los mapas de caracter√≠sticas multidimensionales a un vector unidimensional. Cada neurona en una capa densa est√° conectada a todas las neuronas de la capa anterior, lo que permite que la red combine todas las caracter√≠sticas extra√≠das para tomar decisiones sobre el clasificado final.

Las capas convolucionales aprenden jer√°rquicamente:
-Primeras capas ‚Üí bordes simples
-Capas intermedias ‚Üí formas complejas
-√öltimas capas ‚Üí caracter√≠sticas de alto nivel (ojos, orejas)
    
# Funciones de Activaci√≥n
1. ReLU (Rectified Linear Unit):
```
f(x) = max(0, x)
```
-Ventaja: Introduce no-linealidad, evita el problema de desvanecimiento de gradientes
-Uso: Capas convolucionales y densas intermedias
2. Softmax:
```
f(x_i) = e^(x_i) / Œ£(e^(x_j))
```
-Ventaja: Convierte salidas en probabilidades que suman 1
-Uso: Capa de salida para clasificaci√≥n multiclase

# L√≥gica Difusa (Fuzzy Logic)
A diferencia de la l√≥gica booleana (0 o 1), la l√≥gica difusa permite grados de pertenencia (0 a 1):
Funci√≥n de membres√≠a triangular:
     /\
    /  \
   /    \
  /______\
Se aplica cuando la CNN es incierta (probabilidades cercanas a 0.5), la l√≥gica difusa proporciona un mecanismo para expresar esa incertidumbre.

## Entrenamiento y Validaci√≥n
# Fase 1: Configuraci√≥n inicial
El siguiente bloque desactiva optimizaciones de OneDNN qeu causaron conflictos en su momento, asegurando la compatibilidad de Tensorflow y distintos hardware.
```
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
```
Recomiendo ampliamente crear un entorno virutal con python 3.10.+. La variable [base_dir] es usada para guardar el origen del proyecto, y si se siguen los pasos, las carpetas del dataset tendran esa distribuci√≥n o seguiran ese orden siguiendo rutas relativas m√°s flexibles.
```
base_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
```
# Fase 2: Par√°metros del Modelo
Im√°genes peque√±as (300√ó264) ‚Üí entrenamiento m√°s r√°pido. Recordemos que no es un "resize" como podria pensarse. 
Batch de 32 ‚Üí balance entre estabilidad y velocidad
10 √©pocas ‚Üí suficiente para convergencia inicial
```
#Parametros del modelo
nuevo_alto = 300	
nuevo_ancho = 264	
num_clases = 2
batch_size = 32
num_epocas = 10

```
# Fase 3: Procesamiento de datos
En este punto se normalizan los pixeles de un rango de `[0,255]` a un rango de  `[0,1]` facilitando la convergencia dirante el entrenamiento y evita valores n√∫mericos extremos.
```
datagen = ImageDataGenerator(rescale=1./255)
```

Ahora, [flow_from_directory]: Lee im√°genes directamente desde carpetas, con el proyecto que tiene una estructura esperada como:

conjuntos/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ perros/
‚îÇ   ‚îî‚îÄ‚îÄ gatos/
‚îú‚îÄ‚îÄ val/
|   ‚îú‚îÄ‚îÄ perros/
‚îÇ   ‚îî‚îÄ‚îÄ gatos/
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ perros/
    ‚îî‚îÄ‚îÄ gatos/

En [class_mode='categorical']: Codificaci√≥n one-hot, consiste en una t√©cnica para convertir varias variables categ√≥ricas en un formato n√∫merico qeu los algoritmos de aprendizaje autom√°tico procesan adecuadamente. Consiste en crear una columna binaria para cada c√°tegoria, donde cada columna representa una categoria espec√≠fica y se le atribuye un valor de [1] cuando este pertenece a la cateogor√≠a, [0] en casos contraios (ej: [1,0] para perro, [0,1] para gato).

# Fase 4: Arquitectura del modelo
La l√≠nea siguiente, hacemos que las capas se apilen linealmente una detras de otra. a la salida de una = entrada de la siguiente. 
```
model = Sequential()
```
- Capa de la convoluci√≥n 
Hay distintos elementos, pero en orden, hay 32 filtros que cada uno detecta patrones diferentes, (3,3) es la ventana de convoluci√≥n de 3x3 p√≠xeles, 'relu' introduce la no linealidad, y en `input_shape` tenemos las dimensiones de 300x264 p√≠xeles y 3 canales, cada uno para el RGB
```
Conv2D(32, (3, 3), activation='relu', input_shape=(300, 264, 3))
```
en t√©rminos genrales, la Salida = ReLU(Imagen * Filtro + sesgo)
- Capa 2: Max Pooling
Reduce dimensiones a la mitad (300x264 -> 150x132), pero manteniendo el valor m√°ximo en cada regi√≥n de 2x2. Esto se refleja en un beneficio de reducir par√°metros, acelerar entrenamiento y evitar overfitting.
```
MaxPooling2D(pool_size=(2, 2))
```
- Capa 3: Segunda convolucion
En esta capa 64 filtros detectan patrones m√°s complejos qeu la capa anterior, en lo demas no hay cambios, pero ¬øpor qu√© se necesita aumentar el numero de filtros? A medida que se avanza en la red, se necesita capturar caracter√≠sticas ma abstractas y de mayor nivel. M√°s filtros permiten aprender una mayor variedad de patrones.
```
Conv2D(64, (3, 3), activation='relu')
```

- Capa 4: Segundo Max Pooling
Se reducen las dimensiones a la mitad nuevamente (150x132 -> 75x66) y se aumenta la invariancia a pequelas traslaciones en la imagen.
```
MaxPooling2D(pool_size=(2, 2))
```
- Capa 5: Flatten()
Convierte la salida 3D en un vector 1D y prepara los datos para las capas densas (fully connected).
```
Flatten()
```
- Capa 6: Capa Densa (Fully Connected)
Con 128 Neuronas en las que cada una se conecta a todas las salidas del Flatten. El prop√≥sito es aprender combinaciones complejas de caracteristicas extraidas por als capas convolucionales.
```
Dense(128, activation='relu')
```
- Capa 7: Dropout
En esta capa se regulariza la red, se desactivan aleatoriamente 50% de las neuronas durante el entrenamiento. El beneficio est√° en evitar el overfitting (memorizaci√≥n del dataset de entrenamiento)
```
Dropout(0.5)
```
- Capa 8: Capa de Salida
Se ingresan las calses (2, para perro y gato), softmax convierte las salidas en probabilidades qeu suman 1; ahora la neurona con la mayor probabilidad indica la clase predicha.
```
Dense(num_clases, activation='softmax')
```
# Fase 5: Compilaci√≥n del modelo
Se hace uso de un Algoritmo qeu ajusta las tasas de aprendizaje para cada peso. Loss function `categorical_crossentropy` mide la diferencia entre las predicciones y las etiquetas realies, lo cual es ideal para la clasificacion multiclase con la codificaci√≥n one-hot. Con metrics `accuracy` evaluamos la presici√≥n de las predicciones. 
# Fase 6: Guardar el mejor modelo
Se guarda el modelo durante el entrenamiento, gaurdando solo el que tiene la mejor validaci√≥n, monitoreanoo la p√©rdida en el conjunto de validaci√≥n y guarda el modelo cuando la p√©rdida de la validaci√≥n es m√≠nima
```
checkpoint = ModelCheckpoint('modelo_entrenado.h5', save_best_only=True, monitor='val_loss', mode='min')
```
# Fase 7: Entrenamiento del modelo
En esta etapa tenemos los datos de entrenamiento como [train_generetor], las epocas definidas que son el n√∫mero de pasadas sobre el conjunto de entrenamieneto, los datos de validacion para evaluar el rendimiento del modelo en datos no vistos y usamos el modelCheckpoint para guardar el mejor modelo. 
```
history = model.fit(
    train_generator,
    epochs=num_epocas,
    validation_data=validation_generator,
    callbacks=[checkpoint]
)
```
# Fase 8: Evaluaci√≥n del modelo
En [test_generator] se gaurda un conjunto de daots independiente para evaluar el rendimiento final del modelo (datos de prueba) y en [model.evaluete] se calcula la perdida y la presicion del conjunto de prueba.
```
test_generator = datagen.flow_from_directory(
    os.path.join(base_dir,'conjuntos','test'),
    target_size=(nuevo_alto, nuevo_ancho),
    batch_size=batch_size,
    class_mode='categorical'
)
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Precisi√≥n en el modelo de pruebas: {test_accuracy}')

```
# Fase 9: Guardado del modelo entrenado
En esta fase simplemente se guarda el modelo para su uso posterior.
```
model.save('modelo_entrenado.h5')
```

# Fase 10: Se implementa 
Esta fase introduce un componente adicional para refinar las predicciones de la CNN en situaciones de incertidumbre.

- fuzzy_decision(probabilities)
Funci√≥n de Membres√≠a: Define c√≥mo las probabilidades de la CNN se mapean a grados de pertenencia en conjuntos difusos:
- - low_confidence: Probabilidades bajas (0-0.4)
- - medium_confidence: Probabilidades medias (0.4-0.6)
- - high_confidence: Probabilidades altas (0.6-1)
Reglas Difusas:
- - Si la probabilidad de "perro" es alta (>0.6), la decisi√≥n es "perro".
- - Si la probabilidad de "gato" es alta (>0.6), la decisi√≥n es "gato".
En caso contrario (incertidumbre), la decisi√≥n es "Incierto".
- predict_with_fuzzy_logic(image_path)
- -Carga y preprocesa la imagen.
- - Realiza la predicci√≥n con el modelo CNN.
- - Aplica la l√≥gica difusa para tomar una decisi√≥n final.
- Ejemplo de Predicci√≥n
Demuestra c√≥mo usar la funci√≥n predict_with_fuzzy_logic para predecir la clase de una imagen.

## Conclusi√≥n
Este c√≥digo implementa una CNN para la clasificaci√≥n de im√°genes, utilizando t√©cnicas de regularizaci√≥n y guardado de modelos para optimizar el rendimiento y evitar el overfitting. La adici√≥n de la l√≥gica difusa permite manejar la incertidumbre en las predicciones, proporcionando una toma de decisiones m√°s robusta.

Y si llegaste hasta este punto, creo est√° mas decir que este es mas un proyecto que muestra de manera mas did√°ctica el proceso de hacer una red neuronal mostrando parte de sus conceptos clave. Lo mas improtante es comprender que este proceso puede ser replicado en muchas √°reas con problemas especificos, como en un el √°rea de seguridad e higiene en una empresa, para verificar por medio de c√°maras que los operadores de maquinar√≠a pesada porten adecuadamente su equipo de protecci√≥n, o para verificar que no hay personal en espacios delimitados cuando un operario (de gr√∫a por ejemplo) esta trabajando en ese espacio. En fin, espero que este material sea de ayuda o simplemente aporte en el conocimiento de alguien. 